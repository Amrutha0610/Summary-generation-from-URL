# -*- coding: utf-8 -*-
"""Summary_Generator

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z8XtSixe2PNtR5L32Q-l9AhktPtg59mw
"""

import requests
import nltk
nltk.download('punkt')

import requests
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from collections import defaultdict
import nltk
import math  # Import math module for log function
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from collections import defaultdict, Counter

class Get_Summary:
  def fetch_text(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text,'html.parser')
    text = ' '.join([p.get_text() for p in soup.find_all('p')])
    return text

  def preprocess(url):
    text =  Get_Summary.fetch_text(url)
    words = text.lower().split()  # Convert to lowercase and split into words
    words = [word.strip('.,!?()[]{}') for word in words]  # Strip punctuation
    return words

  #compute term frequency
  def compute_tf(word_dict, words):
    tf_dict = {}
    total_words = len(words)
    for word, count in word_dict.items():
        tf_dict[word] = count / total_words
    return tf_dict


  #inverse document frequency
  def compute_idf(documents):
    N = len(documents)
    idf_dict = {}
    for document in documents:
        for word in document:
            if word in idf_dict:
                continue
            # Count the number of documents containing the word
            count = sum(1 for doc in documents if word in doc)
            idf_dict[word] = math.log(N / (1 + count))
    return idf_dict

  def compute_tfidf(tf, idf):
    tfidf = {}
    for word, tf_value in tf.items():
        tfidf[word] = tf_value * idf.get(word, 0)
    return tfidf

  def extract_keywords(url, num_keywords=50):
    processed_docs = Get_Summary.preprocess(url)

    print(f"preprocessed text is :{processed_docs}")
    # Calculate TF
    word_dict = Counter(processed_docs)
    print(f"word_dict is :{word_dict}")

    #claculate tf
    tf = Get_Summary.compute_tf(word_dict, processed_docs)

    print(f"tf is :{tf}")

    # Calculate IDF
    idf = Get_Summary.compute_idf([word_dict])
    print(f"idf is :{idf}")

    # Calculate TF-IDF
    tfidf = Get_Summary.compute_tfidf(tf, idf)

    # # Calculate TF-IDF
    # tfidf = compute_tfidf(tf, idf)

    # # Get the top keywords
    sorted_keywords = sorted(tfidf.items(), key=lambda item: item[1], reverse=True)
    top_keywords = [keyword for keyword, score in sorted_keywords[:num_keywords]]
    return top_keywords

  def greedy_summary(sentences, keywords):
    selected_sentences = []
    covered_keywords = set()
    remaining_keywords = set(keywords)

    # Greedily select sentences that cover the most uncovered keywords
    while remaining_keywords:
        best_sentence = None
        best_covered = set()

        for sentence in sentences:
            words_in_sentence = set(word_tokenize(sentence.lower()))
            covered_in_sentence = remaining_keywords & words_in_sentence

            if len(covered_in_sentence) > len(best_covered):
                best_covered = covered_in_sentence
                best_sentence = sentence

        if best_sentence:
            selected_sentences.append(best_sentence)
            covered_keywords |= best_covered
            remaining_keywords -= best_covered
        else:
            break

    return selected_sentences

  def generate_summary(url, num_sentences=10, num_keywords=50):
    # Fetch text from URL
    text =  Get_Summary.fetch_text(url)

    # Tokenize into sentences
    sentences = sent_tokenize(text)

    # Extract keywords from the text
    keywords = Get_Summary.extract_keywords(url)

    # Select sentences using greedy approximation for set cover
    summary_sentences = Get_Summary.greedy_summary(sentences, keywords)

    # Print summary
    print("\nSummary:\n")
    for sentence in summary_sentences[:num_sentences]:
        print(sentence)
        print()



# Example usage
if __name__ == "__main__":
    url = "https://en.wikipedia.org/wiki/Artificial_intelligence"  # Example URL
    #result = Get_Summary.fetch_text(url)
    #print(result)
    # preprocessed_text = Get_Summary.preprocess(url)
    # print(preprocessed_text)
    # # #Get_Summary.generate_summary(url)
    #Get_Summary.extract_keywords(url, num_keywords=10)
    #keywords = Get_Summary.extract_keywords(url
    #print(keywords)
    Get_Summary.generate_summary(url)

